import { Injectable, Logger, NotFoundException } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { InjectRepository } from '@nestjs/typeorm';
import { Repository } from 'typeorm';
import { LLMProviderFactory } from '../providers/llm-provider.factory';
import { EncryptionService } from '../../common/services/encryption.service';
import { User } from '../../auth/entity/user.entity';
import { AiSettings } from '../../ai-settings/entity/ai-settings.entity';
import {
  LLMRequest,
  LLMResponse,
  LLMStreamChunk,
  LLMProvider,
} from '../types/llm.types';

/**
 * LLM Adapter Service
 * ì‚¬ìš©ì ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ LLM Providerë¥¼ ì„ íƒí•˜ê³  í˜¸ì¶œí•©ë‹ˆë‹¤.
 */
@Injectable()
export class LLMAdapterService {
  private readonly logger = new Logger(LLMAdapterService.name);

  constructor(
    private readonly providerFactory: LLMProviderFactory,
    private readonly encryptionService: EncryptionService,
    private readonly configService: ConfigService,
    @InjectRepository(User)
    private readonly userRepository: Repository<User>,
    @InjectRepository(AiSettings)
    private readonly aiSettingsRepository: Repository<AiSettings>,
  ) {}

  /**
   * ì‚¬ìš©ì IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
   * @param userId - ì‚¬ìš©ì ID
   * @param messages - ëŒ€í™” ë©”ì‹œì§€ ë°°ì—´
   * @param options - ì¶”ê°€ ì˜µì…˜
   * @returns LLM ì‘ë‹µ
   */
  async generateResponse(
    userId: string,
    messages: Array<{ role: string; content: string }>,
    options?: Partial<LLMRequest>,
  ): Promise<LLMResponse> {
    // ì‚¬ìš©ì ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    const aiSettings = await this.aiSettingsRepository.findOne({
      where: { userId },
    });

    if (!aiSettings) {
      throw new NotFoundException('AI ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }

    // Provider ì„ íƒ
    const provider = this.providerFactory.getProvider(aiSettings.llmProvider);

    // API í‚¤ ê²°ì • (ì‚¬ìš©ì í‚¤ ìš°ì„ , ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ ê¸°ë³¸ê°’)
    const apiKey = await this.resolveApiKey(
      aiSettings.llmProvider,
      userId,
      aiSettings.llmModel,
    );

    // ìš”ì²­ êµ¬ì„±
    const request: LLMRequest = {
      model: aiSettings.llmModel,
      messages,
      temperature: aiSettings.llmConfig?.temperature ?? 0.7,
      maxTokens: aiSettings.llmConfig?.maxTokens ?? 1000,
      topP: aiSettings.llmConfig?.topP,
      topK: aiSettings.llmConfig?.topK,
      frequencyPenalty: aiSettings.llmConfig?.frequencyPenalty,
      presencePenalty: aiSettings.llmConfig?.presencePenalty,
      reasoningEffort: aiSettings.llmConfig?.reasoningEffort,
      ...options,
    };

    // ëª¨ë¸ ê²€ì¦
    if (!provider.validateModel(request.model)) {
      const availableModels = provider.getAvailableModels();
      const errorMsg =
        `ëª¨ë¸ '${request.model}'ì€ Provider '${aiSettings.llmProvider}'ì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ` +
        `ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: ${availableModels.join(', ')}. ` +
        `AI ì„¤ì •ì—ì„œ ì˜¬ë°”ë¥¸ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.`;
      this.logger.error(`âŒ ${errorMsg}`);
      throw new Error(errorMsg);
    }

    // Provider í˜¸ì¶œ
    return provider.generateResponse(request, apiKey);
  }

  /**
   * ì‚¬ìš©ì IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
   * @param userId - ì‚¬ìš©ì ID
   * @param messages - ëŒ€í™” ë©”ì‹œì§€ ë°°ì—´
   * @param onChunk - ê° ì²­í¬ë¥¼ ë°›ì„ ë•Œ í˜¸ì¶œë˜ëŠ” ì½œë°± í•¨ìˆ˜
   * @param options - ì¶”ê°€ ì˜µì…˜
   */
  async generateStreamingResponse(
    userId: string,
    messages: Array<{ role: string; content: string }>,
    onChunk: (chunk: LLMStreamChunk) => void,
    options?: Partial<LLMRequest>,
  ): Promise<void> {
    // ì‚¬ìš©ì ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    const aiSettings = await this.aiSettingsRepository.findOne({
      where: { userId },
    });

    if (!aiSettings) {
      throw new NotFoundException('AI ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }

    // Provider ì„ íƒ
    const provider = this.providerFactory.getProvider(aiSettings.llmProvider);

    // API í‚¤ ê²°ì • (ì‚¬ìš©ì í‚¤ ìš°ì„ , ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ ê¸°ë³¸ê°’)
    const apiKey = await this.resolveApiKey(
      aiSettings.llmProvider,
      userId,
      aiSettings.llmModel,
    );

    // ìš”ì²­ êµ¬ì„±
    const request: LLMRequest = {
      model: aiSettings.llmModel,
      messages,
      temperature: aiSettings.llmConfig?.temperature ?? 0.7,
      maxTokens: aiSettings.llmConfig?.maxTokens ?? 1000,
      topP: aiSettings.llmConfig?.topP,
      topK: aiSettings.llmConfig?.topK,
      frequencyPenalty: aiSettings.llmConfig?.frequencyPenalty,
      presencePenalty: aiSettings.llmConfig?.presencePenalty,
      reasoningEffort: aiSettings.llmConfig?.reasoningEffort,
      stream: true,
      ...options,
    };

    // ëª¨ë¸ ê²€ì¦
    if (!provider.validateModel(request.model)) {
      const availableModels = provider.getAvailableModels();
      const errorMsg =
        `ëª¨ë¸ '${request.model}'ì€ Provider '${aiSettings.llmProvider}'ì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ` +
        `ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: ${availableModels.join(', ')}. ` +
        `AI ì„¤ì •ì—ì„œ ì˜¬ë°”ë¥¸ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.`;
      this.logger.error(`âŒ ${errorMsg}`);
      throw new Error(errorMsg);
    }

    // Provider í˜¸ì¶œ
    return provider.generateStreamingResponse(request, onChunk, apiKey);
  }

  /**
   * API í‚¤ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. (ì‚¬ìš©ì í‚¤ ìš°ì„ , ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ ê¸°ë³¸ê°’)
   * @param provider - Provider íƒ€ì…
   * @param userId - ì‚¬ìš©ì ID
   * @param model - ëª¨ë¸ ì´ë¦„ (ì„ íƒì‚¬í•­)
   * @returns API í‚¤ (ë³µí˜¸í™”ëœ)
   */
  private async resolveApiKey(
    provider: LLMProvider,
    userId: string,
    model?: string,
  ): Promise<string | undefined> {
    try {
      // ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
      const user = await this.userRepository.findOne({
        where: { id: userId },
      });

      if (!user) {
        this.logger.warn(
          `ì‚¬ìš©ì ${userId}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ê¸°ë³¸ í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.`,
        );
        return this.getSystemApiKey(provider);
      }

      // ì‚¬ìš©ìë³„ API í‚¤ í™•ì¸
      this.logger.debug(
        `ğŸ” ì‚¬ìš©ì ${userId}ì˜ API í‚¤ ì¡°íšŒ ì¤‘... Provider: ${provider}`,
      );

      if (user.llmApiKeys) {
        this.logger.debug(
          `ğŸ“¦ ì‚¬ìš©ì API í‚¤ ê°ì²´ ì¡´ì¬: ${JSON.stringify(Object.keys(user.llmApiKeys))}`,
        );

        let encryptedKey: string | undefined;

        switch (provider) {
          case LLMProvider.OPENAI:
            encryptedKey = user.llmApiKeys.openai;
            break;
          case LLMProvider.GOOGLE:
            encryptedKey = user.llmApiKeys.google;
            break;
          case LLMProvider.ANTHROPIC:
            encryptedKey = user.llmApiKeys.anthropic;
            break;
        }

        this.logger.debug(
          `ğŸ”‘ ${provider} Providerì˜ ì•”í˜¸í™”ëœ í‚¤ ì¡´ì¬ ì—¬ë¶€: ${encryptedKey ? 'ìˆìŒ' : 'ì—†ìŒ'}`,
        );

        if (encryptedKey) {
          this.logger.log(
            `ğŸ” ì‚¬ìš©ì ${userId}ì˜ ${provider} ì•”í˜¸í™”ëœ API í‚¤ ë°œê²¬ (ê¸¸ì´: ${encryptedKey.length})`,
          );
          try {
            const decryptedKey =
              this.encryptionService.decryptApiKey(encryptedKey);
            if (decryptedKey && decryptedKey.trim() !== '') {
              this.logger.log(
                `âœ… ì‚¬ìš©ì ${userId}ì˜ ${provider} API í‚¤ ë³µí˜¸í™” ì„±ê³µ (í‚¤ ì‹œì‘: ${decryptedKey.substring(0, Math.min(10, decryptedKey.length))}...)`,
              );
              return decryptedKey;
            } else {
              this.logger.warn(
                `âš ï¸ ì‚¬ìš©ì ${userId}ì˜ ${provider} API í‚¤ ë³µí˜¸í™” ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.`,
              );
            }
          } catch (error) {
            this.logger.error(
              `âŒ ì‚¬ìš©ì ${userId}ì˜ ${provider} API í‚¤ ë³µí˜¸í™” ì‹¤íŒ¨: ${error.message}`,
            );
            this.logger.error(`ë³µí˜¸í™” ì˜¤ë¥˜ ìƒì„¸:`, error);
          }
        } else {
          this.logger.warn(
            `âš ï¸ ì‚¬ìš©ì ${userId}ì˜ ${provider} API í‚¤ê°€ ì €ì¥ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.`,
          );
        }
      } else {
        this.logger.debug(`ì‚¬ìš©ì ${userId}ì˜ llmApiKeysê°€ ì—†ìŠµë‹ˆë‹¤.`);
      }

      // ì‚¬ìš©ì í‚¤ê°€ ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ ê¸°ë³¸ í‚¤ ì‚¬ìš©
      return this.getSystemApiKey(provider);
    } catch (error) {
      this.logger.error(`API í‚¤ ê²°ì • ì‹¤íŒ¨: ${error.message}`);
      return this.getSystemApiKey(provider);
    }
  }

  /**
   * ì‹œìŠ¤í…œ ê¸°ë³¸ API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
   * @param provider - Provider íƒ€ì…
   * @returns API í‚¤
   */
  private getSystemApiKey(provider: LLMProvider): string | undefined {
    switch (provider) {
      case LLMProvider.OPENAI:
        return this.configService.get<string>('OPENAI_API_KEY');
      case LLMProvider.GOOGLE:
        return this.configService.get<string>('GOOGLE_API_KEY');
      case LLMProvider.ANTHROPIC:
        return this.configService.get<string>('ANTHROPIC_API_KEY');
      default:
        return undefined;
    }
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
   * @param provider - Provider íƒ€ì… (ì„ íƒì‚¬í•­, ì—†ìœ¼ë©´ ëª¨ë“  Provider)
   * @returns ëª¨ë¸ ëª©ë¡
   */
  getAvailableModels(provider?: LLMProvider): string[] {
    if (provider) {
      const providerInstance = this.providerFactory.getProvider(provider);
      return providerInstance.getAvailableModels();
    }

    // ëª¨ë“  Providerì˜ ëª¨ë¸ ë°˜í™˜
    const allModels: string[] = [];
    this.providerFactory.getAllProviders().forEach((p) => {
      allModels.push(...p.getAvailableModels());
    });
    return allModels;
  }
}
