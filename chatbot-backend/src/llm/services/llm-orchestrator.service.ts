import { Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { LLMProviderFactory } from '../providers/llm-provider.factory';
import {
  LLMRequest,
  LLMProvider,
  LLMStreamChunk,
} from '../types/llm.types';

/**
 * Multi-Model ìš”ì²­ ì¸í„°í˜ì´ìŠ¤
 */
export interface MultiModelRequest {
  providers: LLMProvider[];
  messages: Array<{ role: string; content: string }>;
  options?: Partial<LLMRequest>;
}

/**
 * ê°œë³„ Provider ì‘ë‹µ
 */
export interface ProviderResponse {
  provider: LLMProvider;
  model: string;
  content: string;
  success: boolean;
  error?: string;
  latency: number;
}

/**
 * Multi-Model ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤
 */
export interface MultiModelResponse {
  responses: ProviderResponse[];
  totalLatency: number;
  successCount: number;
  failCount: number;
}

/**
 * LLM Orchestrator Service
 * ì—¬ëŸ¬ LLM Providerë¥¼ ë™ì‹œì— í˜¸ì¶œí•˜ì—¬ ë³µìˆ˜ì˜ ì‘ë‹µì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
 * ì‚¬ìš©ìëŠ” ì—¬ëŸ¬ AI ëª¨ë¸ì˜ ë‹µë³€ì„ ë¹„êµí•˜ê³  ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
 */
@Injectable()
export class LLMOrchestratorService {
  private readonly logger = new Logger(LLMOrchestratorService.name);

  constructor(
    private readonly providerFactory: LLMProviderFactory,
    private readonly configService: ConfigService,
  ) {}

  /**
   * ì—¬ëŸ¬ LLM Providerë¥¼ ë™ì‹œì— í˜¸ì¶œí•˜ì—¬ ë³µìˆ˜ì˜ ì‘ë‹µì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
   * @param request - Multi-Model ìš”ì²­
   * @param userApiKeys - ì‚¬ìš©ìë³„ API í‚¤ (ì„ íƒì‚¬í•­)
   * @returns Multi-Model ì‘ë‹µ
   */
  async generateMultiModelResponses(
    request: MultiModelRequest,
    userApiKeys?: Partial<Record<LLMProvider, string>>,
  ): Promise<MultiModelResponse> {
    const startTime = Date.now();

    this.logger.log(
      `ğŸš€ Multi-Model ìš”ì²­ ì‹œì‘: ${request.providers.join(', ')}`,
    );

    // ëª¨ë“  Providerë¥¼ ë³‘ë ¬ë¡œ í˜¸ì¶œ
    const promises = request.providers.map((providerType) =>
      this.callProvider(providerType, request, userApiKeys?.[providerType]),
    );

    // ëª¨ë“  ì‘ë‹µ ìˆ˜ì§‘ (ì‹¤íŒ¨í•œ ê²ƒë„ í¬í•¨)
    const results = await Promise.allSettled(promises);

    const responses: ProviderResponse[] = results.map((result, index) => {
      if (result.status === 'fulfilled') {
        return result.value;
      }
      return {
        provider: request.providers[index],
        model: 'unknown',
        content: '',
        success: false,
        error: result.reason?.message || 'Unknown error',
        latency: 0,
      };
    });

    const successCount = responses.filter((r) => r.success).length;
    const failCount = responses.filter((r) => !r.success).length;

    this.logger.log(
      `âœ… Multi-Model ì‘ë‹µ ì™„ë£Œ: ${successCount}/${request.providers.length} ì„±ê³µ`,
    );

    return {
      responses,
      totalLatency: Date.now() - startTime,
      successCount,
      failCount,
    };
  }

  /**
   * ê°œë³„ Providerë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
   */
  private async callProvider(
    providerType: LLMProvider,
    request: MultiModelRequest,
    userApiKey?: string,
  ): Promise<ProviderResponse> {
    const providerStartTime = Date.now();

    try {
      const provider = this.providerFactory.getProvider(providerType);
      const apiKey = userApiKey || this.getSystemApiKey(providerType);

      if (!apiKey) {
        throw new Error(`${providerType} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.`);
      }

      const model = provider.getDefaultModel();

      const llmRequest: LLMRequest = {
        model,
        messages: request.messages,
        temperature: request.options?.temperature ?? 0.7,
        maxTokens: request.options?.maxTokens ?? 1000,
        ...request.options,
      };

      this.logger.debug(`ğŸ“¤ ${providerType} (${model}) í˜¸ì¶œ ì¤‘...`);

      const response = await provider.generateResponse(llmRequest, apiKey);

      this.logger.debug(
        `ğŸ“¥ ${providerType} ì‘ë‹µ ì™„ë£Œ (${Date.now() - providerStartTime}ms)`,
      );

      return {
        provider: providerType,
        model,
        content: response.content,
        success: true,
        latency: Date.now() - providerStartTime,
      };
    } catch (error) {
      this.logger.error(`âŒ ${providerType} í˜¸ì¶œ ì‹¤íŒ¨: ${error.message}`);
      return {
        provider: providerType,
        model: 'unknown',
        content: '',
        success: false,
        error: error.message,
        latency: Date.now() - providerStartTime,
      };
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì˜ Multi-Model í˜¸ì¶œ
   * ê° Providerë³„ë¡œ ê°œë³„ ìŠ¤íŠ¸ë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.
   */
  async generateMultiModelStreams(
    request: MultiModelRequest,
    onChunk: (provider: LLMProvider, chunk: string, model: string) => void,
    onComplete: (provider: LLMProvider, model: string) => void,
    onError: (provider: LLMProvider, error: string) => void,
    userApiKeys?: Partial<Record<LLMProvider, string>>,
  ): Promise<void> {
    this.logger.log(
      `ğŸš€ Multi-Model ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: ${request.providers.join(', ')}`,
    );

    const promises = request.providers.map(async (providerType) => {
      try {
        const provider = this.providerFactory.getProvider(providerType);
        const apiKey =
          userApiKeys?.[providerType] || this.getSystemApiKey(providerType);

        if (!apiKey) {
          throw new Error(`${providerType} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.`);
        }

        const model = provider.getDefaultModel();

        const llmRequest: LLMRequest = {
          model,
          messages: request.messages,
          stream: true,
          temperature: request.options?.temperature ?? 0.7,
          maxTokens: request.options?.maxTokens ?? 1000,
          ...request.options,
        };

        await provider.generateStreamingResponse(
          llmRequest,
          (chunk: LLMStreamChunk) => {
            if (chunk.content) {
              onChunk(providerType, chunk.content, model);
            }
          },
          apiKey,
        );

        onComplete(providerType, model);
      } catch (error) {
        this.logger.error(
          `âŒ ${providerType} ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: ${error.message}`,
        );
        onError(providerType, error.message);
      }
    });

    await Promise.allSettled(promises);
    this.logger.log(`âœ… Multi-Model ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ`);
  }

  /**
   * í•©ì˜ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
   * ì—¬ëŸ¬ ëª¨ë¸ì˜ ë‹µë³€ì„ ì¢…í•©í•˜ì—¬ ìµœì„ ì˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
   */
  async generateConsensusResponse(
    request: MultiModelRequest,
    userApiKeys?: Partial<Record<LLMProvider, string>>,
  ): Promise<{ consensus: string; sources: ProviderResponse[] }> {
    // ë¨¼ì € ëª¨ë“  ëª¨ë¸ì—ì„œ ì‘ë‹µ ìˆ˜ì§‘
    const multiResponse = await this.generateMultiModelResponses(
      request,
      userApiKeys,
    );

    const successfulResponses = multiResponse.responses.filter((r) => r.success);

    if (successfulResponses.length === 0) {
      throw new Error('ëª¨ë“  Provider í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
    }

    if (successfulResponses.length === 1) {
      return {
        consensus: successfulResponses[0].content,
        sources: successfulResponses,
      };
    }

    // GPTì—ê²Œ ì¢…í•© ìš”ì²­
    const consensusPrompt = `ë‹¤ìŒì€ ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•œ ì—¬ëŸ¬ AI ëª¨ë¸ì˜ ë‹µë³€ì…ë‹ˆë‹¤. 
ì´ ë‹µë³€ë“¤ì˜ ê³µí†µì ê³¼ í•µì‹¬ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ê°€ì¥ ì •í™•í•˜ê³  ì™„ì„±ë„ ë†’ì€ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì€ í•œ ë²ˆë§Œ ì–¸ê¸‰í•˜ê³ , ê° ë‹µë³€ì˜ ì¥ì ì„ ì‚´ë ¤ì£¼ì„¸ìš”.

${successfulResponses.map((r) => `[${r.provider.toUpperCase()} - ${r.model}]:\n${r.content}`).join('\n\n---\n\n')}

ìœ„ ë‹µë³€ë“¤ì„ ì¢…í•©í•œ ìµœì¢… ë‹µë³€:`;

    const openaiProvider = this.providerFactory.getProvider(LLMProvider.OPENAI);
    const apiKey =
      userApiKeys?.[LLMProvider.OPENAI] ||
      this.getSystemApiKey(LLMProvider.OPENAI);

    const consensusResponse = await openaiProvider.generateResponse(
      {
        model: openaiProvider.getDefaultModel(),
        messages: [{ role: 'user', content: consensusPrompt }],
        temperature: 0.5,
        maxTokens: 1500,
      },
      apiKey,
    );

    return {
      consensus: consensusResponse.content,
      sources: successfulResponses,
    };
  }

  /**
   * ì‹œìŠ¤í…œ ê¸°ë³¸ API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
   */
  private getSystemApiKey(provider: LLMProvider): string | undefined {
    switch (provider) {
      case LLMProvider.OPENAI:
        return this.configService.get<string>('OPENAI_API_KEY');
      case LLMProvider.GOOGLE:
        return this.configService.get<string>('GOOGLE_API_KEY');
      case LLMProvider.ANTHROPIC:
        return this.configService.get<string>('ANTHROPIC_API_KEY');
      default:
        return undefined;
    }
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ Provider ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
   * API í‚¤ê°€ ì„¤ì •ëœ Providerë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
   */
  getAvailableProviders(): LLMProvider[] {
    const providers: LLMProvider[] = [];

    if (this.getSystemApiKey(LLMProvider.OPENAI)) {
      providers.push(LLMProvider.OPENAI);
    }
    if (this.getSystemApiKey(LLMProvider.GOOGLE)) {
      providers.push(LLMProvider.GOOGLE);
    }
    if (this.getSystemApiKey(LLMProvider.ANTHROPIC)) {
      providers.push(LLMProvider.ANTHROPIC);
    }

    return providers;
  }

  /**
   * Provider ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
   */
  getProviderInfo(): Array<{
    provider: LLMProvider;
    name: string;
    defaultModel: string;
    available: boolean;
  }> {
    return [
      {
        provider: LLMProvider.OPENAI,
        name: 'OpenAI GPT',
        defaultModel: this.providerFactory
          .getProvider(LLMProvider.OPENAI)
          .getDefaultModel(),
        available: !!this.getSystemApiKey(LLMProvider.OPENAI),
      },
      {
        provider: LLMProvider.GOOGLE,
        name: 'Google Gemini',
        defaultModel: this.providerFactory
          .getProvider(LLMProvider.GOOGLE)
          .getDefaultModel(),
        available: !!this.getSystemApiKey(LLMProvider.GOOGLE),
      },
      {
        provider: LLMProvider.ANTHROPIC,
        name: 'Anthropic Claude',
        defaultModel: this.providerFactory
          .getProvider(LLMProvider.ANTHROPIC)
          .getDefaultModel(),
        available: !!this.getSystemApiKey(LLMProvider.ANTHROPIC),
      },
    ];
  }
}

