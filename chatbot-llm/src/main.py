"""
LLM ì„œë²„ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
ëª¨ë“ˆí™”ëœ ì•„í‚¤í…ì²˜ë¡œ êµ¬í˜„ëœ LLM ì„œë²„ì˜ ì§„ì…ì ì…ë‹ˆë‹¤.
"""

import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager

from .config.settings import settings
from .config.logging import setup_logging, get_logger
from .api.chat_routes import router as chat_router
from .api.prompt_routes import router as prompt_router  # í”„ë¡¬í”„íŠ¸ ìƒì„± API ì¶”ê°€
from .api.memory_routes import router as memory_router  # ë©”ëª¨ë¦¬ ê´€ë¦¬ API ì¶”ê°€
from .middleware.logging_middleware import LoggingMiddleware
from .services.llm_service import llm_service


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ ì‹¤í–‰
    logger = get_logger(__name__)
    logger.info("ğŸš€ LLM ì„œë²„ ì‹œì‘ ì¤‘...")
    
    # ë¡œê¹… ì„¤ì •
    setup_logging()
    logger.info("âœ… ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    logger.info("âœ… LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì‹¤í–‰
    logger.info("ğŸ›‘ LLM ì„œë²„ ì¢…ë£Œ ì¤‘...")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    try:
        llm_service.cleanup_memories()
        logger.info("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    logger.info("ğŸ‘‹ LLM ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")


def create_app() -> FastAPI:
    """FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±"""
    
    app = FastAPI(
        title="Chatbot LLM Service",
        description="ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° í”„ë¡¬í”„íŠ¸ ìƒì„± ì„œë¹„ìŠ¤ (LLM í˜¸ì¶œì€ ë°±ì—”ë“œì—ì„œ ì²˜ë¦¬)",
        version="2.0.0",
        lifespan=lifespan
    )
    
    # CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.cors_origins,
        allow_credentials=settings.cors_allow_credentials,
        allow_methods=settings.cors_allow_methods,
        allow_headers=settings.cors_allow_headers,
    )
    
    # ë¡œê¹… ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
    app.add_middleware(LoggingMiddleware)
    
    # ë¼ìš°í„° ë“±ë¡
    # ê¸°ì¡´ ì±„íŒ… API (ì„ íƒì‚¬í•­, í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
    app.include_router(chat_router, prefix="/api")
    # ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ ìƒì„± API
    app.include_router(prompt_router)  # /api/v1/prompt
    # ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ê´€ë¦¬ API
    app.include_router(memory_router)  # /api/v1/memory, /api/v1/context
    
    return app


# ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = create_app()


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "LLM Chat Server",
        "version": "1.0.0",
        "status": "running"
    }


if __name__ == "__main__":
    # ê°œë°œ ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        "src.main:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug,
        log_level=settings.log_level.lower()
    )
